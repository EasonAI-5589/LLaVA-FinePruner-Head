#!/bin/bash

echo "ğŸš€ å¼€å§‹å®Œæ•´çš„æ¨¡å‹è¯„ä¼°æµç¨‹"
echo "è¯„ä¼°é¡ºåº: FastV -> SparseVLM -> PDrop"

# ========== FastV å®Œæ•´è¯„ä¼° ==========
echo "ğŸ“Š 1. å¼€å§‹ FastV å®Œæ•´è¯„ä¼°"

# FastV ä¸åŒtokenæ•°é‡è¯„ä¼°
for task in vqav2 gqa vizwiz sqa textvqa pope mme mmbench mmbench_cn mmvet; do
    for token in 192 128 64; do
        echo "è¿è¡Œ FastV ${task} ${token} tokens"
        CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 bash scripts/v1_5/7b/${task}.sh fastv $token
    done
done

echo "âœ… FastV å®Œæ•´è¯„ä¼°å®Œæˆ"

# ========== SparseVLM å®Œæ•´è¯„ä¼° ==========
echo "ğŸ“Š 2. å¼€å§‹ SparseVLM å®Œæ•´è¯„ä¼°"

for task in vqav2 gqa vizwiz sqa textvqa pope mme mmbench mmbench_cn mmvet; do
    for token in 192 128 64; do
        echo "è¿è¡Œ SparseVLM ${task} ${token} tokens"
        CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 bash scripts/v1_5/7b/${task}.sh sparsevlm $token
    done
done

echo "âœ… SparseVLM å®Œæ•´è¯„ä¼°å®Œæˆ"

# ========== PDrop å®Œæ•´è¯„ä¼° ==========
echo "ğŸ“Š 3. å¼€å§‹ PDrop å®Œæ•´è¯„ä¼°"

for task in vqav2 gqa vizwiz sqa textvqa pope mme mmbench mmbench_cn mmvet; do
    for token in 192 128 64; do
        echo "è¿è¡Œ PDrop ${task} ${token} tokens"
        CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 bash scripts/v1_5/7b/${task}.sh pdrop $token
    done
done

echo "âœ… PDrop å®Œæ•´è¯„ä¼°å®Œæˆ"

echo "ğŸ‰ æ‰€æœ‰æ¨¡å‹è¯„ä¼°å®Œæˆ!"

# ==================== åŸæœ‰è„šæœ¬å†…å®¹ ====================

for task in pope mme vqav2 vizwiz sqa textvqa gqa mmbench mmbench_cn mmvet; do
    for token in 128 64; do
        CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
        bash scripts/v1_5/7b/${task}.sh pdrop $token
    done
done


for task in vqav2 vizwiz mmbench mmbench_cn mmvet; do
    for token in 192 128 64; do
        CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
        bash scripts/v1_5/7b/${task}.sh pdrop $token 32
    done
done

for task in vizwiz; do
    for token in 192 128 64; do
        CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
        bash scripts/v1_5/7b/${task}.sh pdrop $token 32
    done
done


for task in sqa; do
    for token in 192 128 64; do
        CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
        bash scripts/v1_5/7b/${task}.sh sparsevlm $token 32
    done
done


CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/textvqa.sh fastv+finepruner 192 16

CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/gqa.sh fastv+finepruner 192 16

CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/sqa.sh fastv+finepruner 192 16

CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/mme.sh fastv+finepruner 128 8

CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/mme.sh fastv+finepruner 128 16

CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/mme.sh fastv+finepruner 192 8

CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/mme.sh fastv+finepruner 128 24


CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/pope.sh fastv+finepruner 64 8

CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/pope.sh fastv+finepruner 128 8



CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/gqa.sh fastv+finepruner 192 8

CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/gqa.sh fastv+finepruner 128 24


for task in mmbench mmbench_cn; do
    for token in 128 64; do
        for head in 24 16 8; do
            CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/${task}.sh fastv+finepruner "$token" "$head"
        done
    done
done



for task in mmvet; do
  for token in 192 128 64; do
    for head in 24 16 8; do
      CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
      bash scripts/v1_5/7b/${task}.sh fastv+finepruner "$token" "$head"
    done
  done
done

for task in mmbench mmbench_cn; do
  for token in 192 128 64; do
    for head in 24 16 8; do
      CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
      bash scripts/v1_5/7b/${task}.sh fastv+finepruner "$token" "$head"
    done
  done
done

for task in vqav2; do
  for token in 192; do
    for head in 8; do
      CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
      bash scripts/v1_5/7b/${task}.sh fastv+finepruner "$token" "$head"
    done
  done
done


for task in vqav2; do
  for token in 64; do
    for head in 16 8; do
      CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
      bash scripts/v1_5/7b/${task}.sh fastv+finepruner "$token" "$head"
    done
  done
done


CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 bash scripts/v1_5/7b/vqav2.sh fastv+finepruner 128 8


for task in textvqa; do
    for token in 192; do
        for head in 8; do
            CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 bash scripts/v1_5/7b/${task}.sh fastv+finepruner "$token" "$head"
        done
    done
done



# # è¿è¡Œè„šæœ¬
# export PATH=$PATH:/mnt/bn/bes-nas-zqz-lq-v6arnold6/mlx/users/zhangqizhe/env/anaconda3-2024.10/bin
# source activate
# conda activate star

for task in textvqa; do
    for token in 192; do
        for head in 8; do
            CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/7b/${task}.sh fastv+finepruner "$token" "$head"
        done
    done
done




CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 bash scripts/v1_5/7b/mme.sh fastv+finepruner 192 24

# ========== æ¶ˆèç ”ç©¶A: å¤´ç­›é€‰ç­–ç•¥å¯¹æ¯” ==========
echo "ğŸ§ª å¼€å§‹æ¶ˆèç ”ç©¶A: å¤´ç­›é€‰ç­–ç•¥å¯¹æ¯”å®éªŒ"

gpu_list="${CUDA_VISIBLE_DEVICES:-0}"
IFS=',' read -ra GPULIST <<< "$gpu_list"
CHUNKS=${#GPULIST[@]}

CKPT_DIR="/mnt/bn/bes-mllm-shared/checkpoint/LLaVA"
DATA_DIR="/mnt/bn/bes-mllm-shared/data/LLaVA/LLaVA-Eval"
CKPT="llava-v1.5-7b"
SPLIT="llava_pope_test"

# æ¶ˆèç ”ç©¶Aå‚æ•°
METHOD="ablation_a"
TOKEN=128
HEAD=16

# å¤´ç­›é€‰ç­–ç•¥åˆ—è¡¨
STRATEGIES=("sum" "variance" "entropy" "max_attention" "attention_range" "sparsity" "top_k_sum" "weighted_quality" "gini_coefficient")

# åˆå§‹åŒ–ç»“æœæ±‡æ€»æ–‡ä»¶
echo "æ¶ˆèç ”ç©¶A: å¤´ç­›é€‰ç­–ç•¥POPEè¯„ä¼°ç»“æœ" > ./ablation_a_pope_results.txt
echo "æµ‹è¯•æ—¶é—´: $(date)" >> ./ablation_a_pope_results.txt
echo "å‚æ•°: TOKEN=${TOKEN}, HEAD=${HEAD}" >> ./ablation_a_pope_results.txt
echo "========================================" >> ./ablation_a_pope_results.txt
echo "" >> ./ablation_a_pope_results.txt

# è¿è¡Œæ‰€æœ‰å¤´ç­›é€‰ç­–ç•¥
for strategy in "${STRATEGIES[@]}"; do
    echo "æµ‹è¯•å¤´ç­›é€‰ç­–ç•¥: $strategy"

    PARAM="vtn_${TOKEN}_${HEAD}_${strategy}"

    # åˆ›å»ºè¾“å‡ºç›®å½•
    mkdir -p ./playground/data/eval/pope/answers/${SPLIT}/${CKPT}/${METHOD}/${PARAM}

    # å¹¶è¡Œè¿è¡Œå¤šGPU
    for IDX in $(seq 0 $((CHUNKS-1))); do
        CUDA_VISIBLE_DEVICES=${GPULIST[$IDX]} python -m llava.eval.model_vqa_loader \
            --model-path ${CKPT_DIR}/${CKPT} \
            --question-file ./playground/data/eval/pope/${SPLIT}.jsonl \
            --image-folder ${DATA_DIR}/pope/val2014 \
            --answers-file ./playground/data/eval/pope/answers/${SPLIT}/${CKPT}/${METHOD}/${PARAM}/${CHUNKS}_${IDX}.jsonl \
            --num-chunks ${CHUNKS} \
            --chunk-idx ${IDX} \
            --pruning_method ${METHOD} \
            --visual_token_num ${TOKEN} \
            --H ${HEAD} \
            --head-selection-strategy ${strategy} \
            --temperature 0 \
            --conv-mode vicuna_v1 &
    done

    wait

    # åˆå¹¶ç»“æœ
    output_file=./playground/data/eval/pope/answers/${SPLIT}/${CKPT}/${METHOD}/${PARAM}/merge.jsonl
    > "$output_file"

    for IDX in $(seq 0 $((CHUNKS-1))); do
        cat ./playground/data/eval/pope/answers/${SPLIT}/${CKPT}/${METHOD}/${PARAM}/${CHUNKS}_${IDX}.jsonl >> "$output_file"
    done

    # è¯„ä¼°ç»“æœ
    echo "" >> ./ablation_a_pope_results.txt
    echo "ç­–ç•¥: $strategy" >> ./ablation_a_pope_results.txt
    python llava/eval/eval_pope.py \
        --annotation-dir ${DATA_DIR}/pope/coco \
        --question-file ./playground/data/eval/pope/${SPLIT}.jsonl \
        --result-file $output_file >> ./ablation_a_pope_results.txt
    echo "----------------------------------------" >> ./ablation_a_pope_results.txt

    echo "âœ… ç­–ç•¥ $strategy å®Œæˆ!"
done

echo "âœ… æ¶ˆèç ”ç©¶Aå®éªŒå®Œæˆ!"
echo ""
echo "ğŸ“Š æ‰€æœ‰ç­–ç•¥è¯„ä¼°ç»“æœå·²æ±‡æ€»åˆ°: ./ablation_a_pope_results.txt"
echo "å¯ä»¥ç›´æ¥æŸ¥çœ‹æ¯”è¾ƒå„ç­–ç•¥æ•ˆæœ:"
echo "cat ./ablation_a_pope_results.txt"

# ========== MME è¯„ä¼° ==========
echo "ğŸ§ª å¼€å§‹MMEè¯„ä¼°å®éªŒ"

gpu_list="${CUDA_VISIBLE_DEVICES:-0}"
IFS=',' read -ra GPULIST <<< "$gpu_list"
CHUNKS=${#GPULIST[@]}

CKPT_DIR="/mnt/bn/bes-mllm-shared/checkpoint/LLaVA"
DATA_DIR="/mnt/bn/bes-mllm-shared/data/LLaVA/LLaVA-Eval"
CKPT="llava-v1.5-7b"
SPLIT="llava_mme"

METHOD="fastv+finepruner"
TOKEN=192
HEAD=24
PARAM="vtn_${TOKEN}_${HEAD}"

echo "MMEè¯„ä¼°å‚æ•°: METHOD=${METHOD}, TOKEN=${TOKEN}, HEAD=${HEAD}"

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p ./playground/data/eval/MME/answers/${SPLIT}/${CKPT}/${METHOD}/${PARAM}

# å¹¶è¡Œè¿è¡Œå¤šGPU
for IDX in $(seq 0 $((CHUNKS-1))); do
    CUDA_VISIBLE_DEVICES=${GPULIST[$IDX]} python -m llava.eval.model_vqa_loader \
        --model-path ${CKPT_DIR}/${CKPT} \
        --question-file ./playground/data/eval/MME/${SPLIT}.jsonl \
        --image-folder ${DATA_DIR}/MME/MME_Benchmark_release_version \
        --answers-file ./playground/data/eval/MME/answers/${SPLIT}/${CKPT}/${METHOD}/${PARAM}/${CHUNKS}_${IDX}.jsonl \
        --num-chunks ${CHUNKS} \
        --chunk-idx ${IDX} \
        --pruning_method ${METHOD} \
        --visual_token_num ${TOKEN} \
        --H ${HEAD} \
        --temperature 0 \
        --conv-mode vicuna_v1 &
done

wait

# åˆå¹¶ç»“æœ
output_file=./playground/data/eval/MME/answers/${SPLIT}/${CKPT}/${METHOD}/${PARAM}/merge.jsonl
> "$output_file"

for IDX in $(seq 0 $((CHUNKS-1))); do
    cat ./playground/data/eval/MME/answers/${SPLIT}/${CKPT}/${METHOD}/${PARAM}/${CHUNKS}_${IDX}.jsonl >> "$output_file"
done

# è½¬æ¢ç­”æ¡ˆæ ¼å¼
cd ./playground/data/eval/MME

python convert_answer_to_mme.py \
    --data_path ${DATA_DIR}/MME \
    --experiment ${SPLIT}/${CKPT}/${METHOD}/${PARAM}/merge

# è®¡ç®—è¯„ä¼°ç»“æœ
cd eval_tool

python calculation.py --results_dir answers/${SPLIT}/${CKPT}/${METHOD}/${PARAM}

echo "âœ… MMEè¯„ä¼°å®Œæˆ!"