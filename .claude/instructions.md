# 特定指令和工作规范

## 代码修改规范

### 代码风格要求
- 保持现有的代码缩进和格式风格
- 使用与项目一致的变量命名规范
- 添加必要的类型注解和文档字符串
- 遵循Python PEP8规范，行长度限制100字符

### 模型相关修改
- 修改核心模型文件时必须保持向后兼容性
- 新增参数需要提供默认值
- 关键算法变更需要添加详细注释
- 性能优化要保持数值稳定性

## 实验运行规范

### GPU资源管理
- 运行实验前检查GPU内存使用: `nvidia-smi`
- 大规模实验使用8卡并行: `CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7`
- 小规模测试可使用单卡: `CUDA_VISIBLE_DEVICES=0`
- 避免在运行实验时同时启动多个大任务

### 实验数据管理
- 所有实验结果保存到对应的results目录
- 使用标准化的文件命名格式: `[method]_[dataset]_[tokens]_[heads].txt`
- 重要实验结果及时备份到git仓库
- 删除过期的临时实验文件节省存储空间

### 评估流程规范
- 运行完整评估前先进行小规模测试验证
- 批量实验使用脚本自动化运行，避免手动操作
- 实验运行时监控日志输出，及时发现异常
- 评估完成后立即检查结果文件完整性

## 代码提交规范

### 提交前检查
- 运行类型检查: `python -m mypy llava/ --ignore-missing-imports`
- 代码格式化: `black llava/ --line-length 100`
- 确保关键功能测试通过
- 检查是否有调试代码和打印语句遗留

### Git工作流
- 基于功能创建分支进行开发
- 提交信息使用清晰的描述: `feat: 实现动态头选择策略`
- 避免提交大型二进制文件和临时文件
- 定期同步main分支的最新变更

## 性能优化指令

### 内存优化
- 大批量实验时注意清理GPU内存
- 使用gradient checkpointing减少内存占用
- 避免在循环中创建大量临时对象
- 及时释放不需要的tensor和变量

### 计算优化
- 利用torch.compile加速模型推理
- 使用混合精度训练和推理
- 批处理相似大小的样本减少padding
- 缓存重复计算的中间结果

## 调试和故障排除

### 常见问题处理
- CUDA内存不足: 减少batch size或token数量
- 模型加载失败: 检查checkpoint路径和模型版本
- 评估脚本报错: 确认数据集路径和格式正确
- 结果文件缺失: 检查权限和磁盘空间

### 日志和监控
- 使用详细的日志记录关键步骤
- 监控实验进度避免浪费计算资源
- 保存异常情况的错误日志便于分析
- 定期检查系统资源使用情况

## 文档和报告

### 实验记录
- 记录重要实验的参数配置和结果
- 对比实验要使用相同的基准设置
- 异常结果需要分析原因并记录
- 成功的优化经验要及时总结分享

### 代码文档
- 复杂算法添加详细的实现说明
- 参数含义和取值范围要明确标注
- 提供使用示例和预期输出格式
- 更新README和技术文档保持同步