#!/usr/bin/env python3
"""
Head Strategy Evaluation Results Parser

This script parses evaluation results from multiple benchmarks (TextVQA, MME, GQA, POPE)
and generates comprehensive markdown tables for analysis.

Usage:
    python parse_evaluation_results.py

Output:
    - Parses all four evaluation result files
    - Generates markdown tables for each benchmark
    - Creates comprehensive analysis report
"""

import re
import json
from pathlib import Path

def parse_textvqa_results(filepath):
    """Parse TextVQA evaluation results."""
    results = {}
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    # Pattern to match each result block with accuracy
    pattern = r'TOKEN=(\d+), HEAD=(\d+), STRATEGY=([^:]+):\s*merge\s*Samples: \d+\s*Accuracy: ([\d.]+)%'
    matches = re.findall(pattern, content)

    for token, head, strategy, accuracy in matches:
        token = int(token)
        head = int(head)
        accuracy = float(accuracy)

        if token not in results:
            results[token] = {}
        if head not in results[token]:
            results[token][head] = {}

        results[token][head][strategy] = accuracy

    return results

def parse_mme_results(filepath):
    """Parse MME evaluation results."""
    results = {}
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    # Split content by TOKEN= to avoid cross-boundary matching
    sections = content.split('TOKEN=')[1:]  # Skip empty first element

    print(f"MMEæ‰¾åˆ° {len(sections)} ä¸ªé…ç½®æ®µè½")

    for section in sections:
        # Add back the TOKEN= prefix for parsing
        section = 'TOKEN=' + section

        # Extract header info
        header_match = re.match(r'TOKEN=(\d+), HEAD=(\d+), STRATEGY=([^:]+):', section)
        if not header_match:
            continue

        token = int(header_match.group(1))
        head = int(header_match.group(2))
        strategy = header_match.group(3)

        # Look for the ALL section total score
        all_match = re.search(r'=========== ALL ===========\s*total score:\s*([\d.]+)', section)
        if not all_match:
            print(f"è­¦å‘Š: {token}/{head}/{strategy} æ²¡æœ‰æ‰¾åˆ°ALLæ€»åˆ†")
            continue

        total_score = float(all_match.group(1))

        # Store result
        if token not in results:
            results[token] = {}
        if head not in results[token]:
            results[token][head] = {}

        results[token][head][strategy] = total_score

        # Debug output for previously problematic configs
        if (token == 128 and head == 24 and strategy == 'max_attention') or \
           (token == 64 and head == 24 and strategy == 'max_attention'):
            print(f"æˆåŠŸè§£æ: {token}/{head}/{strategy} = {total_score}")

    print(f"MMEè§£æå®Œæˆï¼Œå…± {sum(len(head_data) for token_data in results.values() for head_data in token_data.values())} ä¸ªé…ç½®")
    return results

def parse_gqa_results(filepath):
    """Parse GQA evaluation results."""
    results = {}
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    # Pattern to match each result block with accuracy
    pattern = r'TOKEN=(\d+), HEAD=(\d+), STRATEGY=([^:]+):\s*.*?Accuracy: ([\d.]+)%'
    matches = re.findall(pattern, content, re.DOTALL)

    for token, head, strategy, accuracy in matches:
        token = int(token)
        head = int(head)
        accuracy = float(accuracy)

        if token not in results:
            results[token] = {}
        if head not in results[token]:
            results[token][head] = {}

        results[token][head][strategy] = accuracy

    return results

def parse_pope_results(filepath):
    """Parse POPE evaluation results."""
    results = {}
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    # Pattern to match each result block with Average F1 score
    pattern = r'TOKEN=(\d+), HEAD=(\d+), STRATEGY=([^:]+):.*?Average F1 score: ([\d.]+)'
    matches = re.findall(pattern, content, re.DOTALL)

    for token, head, strategy, f1_score in matches:
        token = int(token)
        head = int(head)
        f1_score = float(f1_score) * 100  # Convert to percentage

        if token not in results:
            results[token] = {}
        if head not in results[token]:
            results[token][head] = {}

        results[token][head][strategy] = f1_score

    return results

def generate_markdown_table(results, metric_name, format_func=None):
    """Generate markdown table from results."""
    if not results:
        return f"## {metric_name}\n\nNo data available.\n"

    # Get all strategies
    strategies = set()
    for token_data in results.values():
        for head_data in token_data.values():
            strategies.update(head_data.keys())

    strategies = sorted(list(strategies))

    # Create table header
    header = f"| Token | Head | " + " | ".join(strategies) + " |"
    separator = "|" + "---|" * (len(strategies) + 2)

    rows = [f"## {metric_name}", "", header, separator]

    # Generate table with highlighting
    for token in sorted(results.keys(), reverse=True):
        for head in sorted(results[token].keys(), reverse=True):
            row_data = [f"**{token}**" if head == max(results[token].keys()) else "", str(head)]

            # Find best score for this specific row (token/head combination)
            row_scores = {}
            for strategy in strategies:
                if strategy in results[token][head]:
                    row_scores[strategy] = results[token][head][strategy]

            # Find the best strategy for this row
            best_strategy = None
            if row_scores:
                best_strategy = max(row_scores, key=row_scores.get)

            for strategy in strategies:
                if strategy in results[token][head]:
                    score = results[token][head][strategy]

                    if format_func:
                        formatted_score = format_func(score)
                    else:
                        formatted_score = f"{score:.2f}%"

                    # Highlight if this is the best score for this row
                    if strategy == best_strategy:
                        formatted_score = f"**{formatted_score}**"

                    row_data.append(formatted_score)
                else:
                    row_data.append("-")

            row = "| " + " | ".join(row_data) + " |"
            rows.append(row)

    return "\n".join(rows) + "\n"

def generate_summary_analysis(textvqa_results, mme_results, gqa_results, pope_results):
    """Generate summary analysis of all results."""
    analysis = [
        "## ä¸»è¦å‘ç°",
        "",
        "### å„è¯„ä¼°æŒ‡æ ‡æœ€ä½³é…ç½®",
        ""
    ]

    # Find best configurations for each metric
    metrics = [
        ("TextVQA", textvqa_results, lambda x: f"{x:.2f}%"),
        ("MME", mme_results, lambda x: f"{x:.1f}"),
        ("GQA", gqa_results, lambda x: f"{x:.2f}%"),
        ("POPE", pope_results, lambda x: f"{x:.2f}%")
    ]

    for metric_name, results, format_func in metrics:
        if not results:
            continue

        best_score = 0
        best_config = None

        for token in results:
            for head in results[token]:
                for strategy, score in results[token][head].items():
                    if score > best_score:
                        best_score = score
                        best_config = (token, head, strategy)

        if best_config:
            token, head, strategy = best_config
            analysis.append(f"- **{metric_name}**: Token={token}, Head={head}, Strategy={strategy} â†’ {format_func(best_score)}")

    # Analyze strategy performance across different configurations
    analysis.extend([
        "",
        "### Head Selectionç­–ç•¥åˆ†æ",
        "",
        "æ ¹æ®æ¯è¡Œæœ€ä¼˜ç»“æœçš„ç»Ÿè®¡åˆ†æï¼š",
        ""
    ])

    # Count wins for each strategy across all configurations
    strategy_wins = {}
    all_results = [("TextVQA", textvqa_results), ("MME", mme_results), ("GQA", gqa_results), ("POPE", pope_results)]

    for metric_name, results in all_results:
        if not results:
            continue

        for token in results:
            for head in results[token]:
                # Find best strategy for this configuration
                best_score = 0
                best_strategy = None
                for strategy, score in results[token][head].items():
                    if score > best_score:
                        best_score = score
                        best_strategy = strategy

                if best_strategy:
                    key = f"{token}/{head}"
                    if best_strategy not in strategy_wins:
                        strategy_wins[best_strategy] = []
                    strategy_wins[best_strategy].append(f"{metric_name}-{key}")

    # Sort strategies by number of wins
    sorted_strategies = sorted(strategy_wins.items(), key=lambda x: len(x[1]), reverse=True)

    analysis.append("**ç­–ç•¥ä¼˜åŠ¿åˆ†æï¼š**")
    for strategy, wins in sorted_strategies:
        analysis.append(f"- **{strategy}**: åœ¨ {len(wins)} ä¸ªé…ç½®ä¸­è¡¨ç°æœ€ä½³")

    analysis.extend([
        "",
        "**ç­–ç•¥ç‰¹ç‚¹ï¼š**",
        "- **sparsity**: åœ¨é«˜tokenæ•°(192,128)å’Œè¾ƒå¤šheadæ•°(24,16)æ—¶è¡¨ç°æœ€ä½³ï¼Œæ˜¯è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½çš„å¥½å¹³è¡¡",
        "- **graph_based**: åœ¨ä½tokenæ•°(64)æ—¶è¡¨ç°çªå‡ºï¼Œç‰¹åˆ«é€‚åˆèµ„æºå—é™åœºæ™¯",
        "- **top_k_sum**: åœ¨8ä¸ªheadçš„é…ç½®ä¸‹ç»å¸¸æ˜¯æœ€ä¼˜é€‰æ‹©ï¼Œé€‚åˆæåº¦å‹ç¼©çš„åœºæ™¯",
        "- **hierarchical**: åœ¨ç‰¹å®šé«˜è´¨é‡é…ç½®(192/16)ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯æ¨ç†ç±»ä»»åŠ¡",
        "",
        "### æ¨èé…ç½®",
        "",
        "**æ ¹æ®èµ„æºçº¦æŸçš„æ¨èç­–ç•¥ï¼š**",
        "",
        "1. **å……è¶³èµ„æºåœºæ™¯** (Tokenâ‰¥192, Headâ‰¥16):",
        "   - æ¨èç­–ç•¥: **sparsity** æˆ– **hierarchical**",
        "   - ç†ç”±: åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½èƒ½è¾¾åˆ°æœ€ä¼˜æˆ–æ¥è¿‘æœ€ä¼˜æ€§èƒ½",
        "",
        "2. **ä¸­ç­‰èµ„æºåœºæ™¯** (Token=128, Head=16-24):",
        "   - æ¨èç­–ç•¥: **sparsity**",
        "   - ç†ç”±: åœ¨å¤šæ•°æƒ…å†µä¸‹è¡¨ç°æœ€ä½³ï¼Œå…¼é¡¾æ•ˆç‡å’Œæ€§èƒ½",
        "",
        "3. **å—é™èµ„æºåœºæ™¯** (Tokenâ‰¤64 æˆ– Headâ‰¤8):",
        "   - æ¨èç­–ç•¥: **graph_based** æˆ– **top_k_sum**",
        "   - ç†ç”±: åœ¨ä½èµ„æºé…ç½®ä¸‹è¡¨ç°ç¨³å®šï¼Œé€‚åˆè¾¹ç¼˜éƒ¨ç½²",
        "",
        "**æ€»ä½“æ¨è: sparsityç­–ç•¥** - åœ¨å¤§å¤šæ•°é…ç½®ä¸‹éƒ½èƒ½æä¾›ç¨³å®šçš„é«˜æ€§èƒ½è¡¨ç°ï¼Œæ˜¯æœ€versatileçš„é€‰æ‹©ã€‚"
    ])

    return "\n".join(analysis)

def main():
    """Main function to parse all results and generate markdown report."""
    base_path = Path(".")

    # File paths
    files = {
        'textvqa': base_path / "head_strategy_textvqa_results.txt",
        'mme': base_path / "head_strategy_mme_results.txt",
        'gqa': base_path / "head_strategy_gqa_results.txt",
        'pope': base_path / "head_strategy_pope_results.txt"
    }

    # Parse results
    print("ğŸ” è§£æè¯„ä¼°ç»“æœæ–‡ä»¶...")
    textvqa_results = parse_textvqa_results(files['textvqa']) if files['textvqa'].exists() else {}
    mme_results = parse_mme_results(files['mme']) if files['mme'].exists() else {}
    gqa_results = parse_gqa_results(files['gqa']) if files['gqa'].exists() else {}
    pope_results = parse_pope_results(files['pope']) if files['pope'].exists() else {}

    # Generate markdown report
    print("ğŸ“Š ç”ŸæˆMarkdownæŠ¥å‘Š...")
    report_parts = [
        "# Head Strategy Evaluation Results Analysis",
        "",
        "è¯„ä¼°æ—¶é—´: Fri 19 Sep 2025",
        "æ–¹æ³•: ablation_a",
        "æµ‹è¯•Tokenæ•°é‡: 192, 128, 64",
        "æµ‹è¯•Headæ•°é‡: 24, 16, 8",
        "æµ‹è¯•ç­–ç•¥: max_attention, attention_range, sparsity, top_k_sum, multi_objective, graph_based, hierarchical",
        "",
        generate_markdown_table(textvqa_results, "TextVQAè¯„ä¼°ç»“æœ"),
        generate_markdown_table(mme_results, "MMEè¯„ä¼°ç»“æœ", lambda x: f"{x:.1f}"),
        generate_markdown_table(gqa_results, "GQAè¯„ä¼°ç»“æœ"),
        generate_markdown_table(pope_results, "POPEè¯„ä¼°ç»“æœ"),
        generate_summary_analysis(textvqa_results, mme_results, gqa_results, pope_results)
    ]

    # Write report
    output_file = base_path / "head_strategy_analysis_report.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("\n".join(report_parts))

    print(f"âœ… åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
    print(f"ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"  - TextVQA: {sum(len(head_data) for token_data in textvqa_results.values() for head_data in token_data.values())} ä¸ªé…ç½®")
    print(f"  - MME: {sum(len(head_data) for token_data in mme_results.values() for head_data in token_data.values())} ä¸ªé…ç½®")
    print(f"  - GQA: {sum(len(head_data) for token_data in gqa_results.values() for head_data in token_data.values())} ä¸ªé…ç½®")
    print(f"  - POPE: {sum(len(head_data) for token_data in pope_results.values() for head_data in token_data.values())} ä¸ªé…ç½®")

if __name__ == "__main__":
    main()