import torch
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union
from transformers.models.llama import LlamaConfig
from transformers.models.llama.modeling_llama import LlamaModel, Cache, DynamicCache, \
    _prepare_4d_causal_attention_mask, _prepare_4d_causal_attention_mask_for_sdpa
from transformers.modeling_outputs import BaseModelOutputWithPast


R_dict = {
    "7b": {
        2: {
            192: 166,
            128: 98,
            64: 30,
        },
        3: {
            192: 152,
            128: 82,
            64: 11,
        },
    },
    "13b": {
        2: {
            192: 172,
            128: 104,
            64: 37,
        },
        3: {
            192: 161,
            128: 92,
            64: 22,
        }
    }
}


class FineFastVLlamaModelAblationDynamic(LlamaModel):
    """
    åŠ¨æ€Headé€‰æ‹©ç­–ç•¥: å±‚æ¬¡åŒ–æ™ºèƒ½é€‰æ‹©
    Step 1: ç²—ç­›é€‰ - æ£€æµ‹è§†è§‰æ•æ„Ÿå¤´
    Step 2: ç­–ç•¥åˆ¤æ–­ - æ™ºèƒ½é€‰æ‹©ç­›é€‰ç­–ç•¥
    Step 3: ç²¾ç­›é€‰ - åº”ç”¨ç­–ç•¥ç²¾ç»†ç­›é€‰
    Step 4: è‡ªé€‚åº”æ•°é‡ - åŠ¨æ€è°ƒæ•´å¤´æ•°é‡
    """

    def __init__(self, config: LlamaConfig, visual_token_num: int):
        super().__init__(config)
        self.system_prompt_length = 35
        self.visual_token_num = 0

        if config.num_hidden_layers == 32:
            self.scale = "7b"
        elif config.num_hidden_layers == 40:
            self.scale = "13b"

        if config.image_aspect_ratio == "pad":
            self.visual_token_length = 576
            self.anyres = False
        elif config.image_aspect_ratio == "anyres":
            self.visual_token_length = 2880
            self.anyres = True

        # FastV config
        self.K = 2
        self.R = R_dict[self.scale][self.K][visual_token_num]
        if self.anyres:
            self.R *= 5
        self.H = 32  # åŸºç¡€ç›®æ ‡å¤´æ•°é‡ï¼Œå°†åŠ¨æ€è°ƒæ•´
        self.remove_sink = False

        # åŠ¨æ€é€‰æ‹©é…ç½®
        self.enable_dynamic_selection = getattr(config, 'enable_dynamic_selection', True)
        self.min_heads = getattr(config, 'min_heads', 8)
        self.max_heads = getattr(config, 'max_heads', 24)
        self.debug_mode = False  # ç®€åŒ–ï¼Œé»˜è®¤å…³é—­debug

        print(f"ğŸ”§ Dynamic Head Selection initialized: enabled={self.enable_dynamic_selection}")
        print(f"   Target heads: {self.H}, Range: [{self.min_heads}, {self.max_heads}]")

    def get_last_query_attention(self, last_attention):
        """å›ºå®šä½¿ç”¨æœ€åä¸€ä¸ªquery tokençš„æ³¨æ„åŠ›"""
        return last_attention[0, :, -1, self.system_prompt_length:self.system_prompt_length+self.visual_token_length]

    # ================== Step 1: ç²—ç­›é€‰ - è§†è§‰æ•æ„Ÿåº¦æ£€æµ‹ ==================

    def dynamic_visual_head_detection(self, image_attention):
        """
        ç²—ç­›é€‰ï¼šæ£€æµ‹å¯¹è§†è§‰ä¿¡æ¯æ•æ„Ÿçš„æ³¨æ„åŠ›å¤´

        è¾“å…¥: image_attention (32, 576)
        è¾“å‡º: visual_head_indices, head_sensitivity_scores
        """
        H, N = image_attention.shape  # (32, 576)

        # æŒ‡æ ‡1: æ€»æ³¨æ„åŠ›å¼ºåº¦ - è¡¡é‡å¤´å¯¹è§†è§‰tokençš„æ€»ä½“å…³æ³¨åº¦
        total_attention = image_attention.sum(dim=-1)  # (32,)

        # æŒ‡æ ‡2: æ³¨æ„åŠ›æ–¹å·® - è¡¡é‡æ³¨æ„åŠ›åˆ†å¸ƒçš„ä¸å‡åŒ€æ€§
        attention_variance = image_attention.var(dim=-1)  # (32,)

        # æŒ‡æ ‡3: æœ€å¤§æ³¨æ„åŠ›å€¼ - è¡¡é‡æ˜¯å¦æœ‰å¼ºçƒˆçš„å…³æ³¨ç‚¹
        max_attention = image_attention.max(dim=-1)[0]  # (32,)

        # æŒ‡æ ‡4: æœ‰æ•ˆå…³æ³¨èŒƒå›´ - è¡¡é‡å…³æ³¨çš„visual tokenæ•°é‡
        mean_attention = image_attention.mean(dim=-1, keepdim=True)  # (32, 1)
        effective_tokens = (image_attention > mean_attention).float().sum(dim=-1)  # (32,)

        # æŒ‡æ ‡5: æ³¨æ„åŠ›é›†ä¸­åº¦ - top-10%çš„æ³¨æ„åŠ›å æ¯”
        topk_size = max(1, N // 10)  # top 10%
        topk_attention = image_attention.topk(k=topk_size, dim=-1)[0].sum(dim=-1)  # (32,)
        concentration = topk_attention / (total_attention + 1e-8)  # (32,)

        # å½’ä¸€åŒ–å„æŒ‡æ ‡åˆ°[0,1]
        def normalize_score(score):
            return (score - score.min()) / (score.max() - score.min() + 1e-8)

        total_norm = normalize_score(total_attention)
        variance_norm = normalize_score(attention_variance)
        max_norm = normalize_score(max_attention)
        effective_norm = normalize_score(effective_tokens)
        concentration_norm = normalize_score(concentration)

        # ç»¼åˆè§†è§‰æ•æ„Ÿåº¦è¯„åˆ† (æƒé‡å¯è°ƒ)
        visual_sensitivity = (
            0.25 * total_norm +           # æ€»å…³æ³¨åº¦
            0.20 * variance_norm +        # åˆ†å¸ƒä¸å‡åŒ€æ€§
            0.25 * max_norm +             # peakå…³æ³¨åº¦
            0.15 * effective_norm +       # å…³æ³¨å¹¿åº¦
            0.15 * concentration_norm     # é›†ä¸­åº¦
        )

        # åŠ¨æ€é˜ˆå€¼ç­–ç•¥
        # æ–¹æ³•1: ç»Ÿè®¡é˜ˆå€¼
        mean_sens = visual_sensitivity.mean()
        std_sens = visual_sensitivity.std()
        threshold_1 = mean_sens + 0.5 * std_sens

        # æ–¹æ³•2: åˆ†ä½æ•°é˜ˆå€¼
        threshold_2 = torch.quantile(visual_sensitivity, 0.6)  # ä¿ç•™top 40%

        # æ–¹æ³•3: åŠ¨æ€gapæ£€æµ‹
        sorted_sens, _ = torch.sort(visual_sensitivity, descending=True)
        if len(sorted_sens) > 1:
            gaps = sorted_sens[:-1] - sorted_sens[1:]  # è®¡ç®—ç›¸é‚»åˆ†æ•°çš„gap
            max_gap_idx = gaps.argmax().item()
            threshold_3 = sorted_sens[max_gap_idx + 1]
        else:
            threshold_3 = threshold_1

        # é€‰æ‹©æœ€åˆé€‚çš„é˜ˆå€¼
        final_threshold = max(threshold_1, threshold_2, threshold_3)

        # è·å–è¶…è¿‡é˜ˆå€¼çš„å¤´
        visual_heads_mask = visual_sensitivity > final_threshold
        visual_head_indices = torch.where(visual_heads_mask)[0]

        # ä¿è¯æœ€å°‘æ•°é‡ (é¿å…é€‰æ‹©è¿‡å°‘)
        if len(visual_head_indices) < self.min_heads:
            visual_head_indices = visual_sensitivity.topk(k=self.min_heads).indices

        # ä¿è¯æœ€å¤šæ•°é‡ (é¿å…é€‰æ‹©è¿‡å¤š)
        if len(visual_head_indices) > self.max_heads:
            selected_scores = visual_sensitivity[visual_head_indices]
            top_indices = selected_scores.topk(k=self.max_heads).indices
            visual_head_indices = visual_head_indices[top_indices]

        if self.debug_mode:
            print(f"   Visual sensitivity stats: mean={mean_sens:.3f}, std={std_sens:.3f}")
            print(f"   Thresholds: stat={threshold_1:.3f}, quantile={threshold_2:.3f}, gap={threshold_3:.3f}")
            print(f"   Selected {len(visual_head_indices)}/32 visual-sensitive heads")

        return visual_head_indices, visual_sensitivity[visual_head_indices]

    # ================== Step 2: ç­–ç•¥åˆ¤æ–­ - æ™ºèƒ½ç­–ç•¥é€‰æ‹© ==================

    def adaptive_strategy_selection(self, image_attention, visual_head_indices):
        """
        ç­–ç•¥åˆ¤æ–­ï¼šåˆ†ææ³¨æ„åŠ›æ¨¡å¼ï¼Œé€‰æ‹©æœ€é€‚åˆçš„ç²¾ç­›ç­–ç•¥

        è¾“å…¥: image_attention (32, 576), visual_head_indices (M,)
        è¾“å‡º: best_strategy (str)
        """
        selected_attention = image_attention[visual_head_indices]  # (M, 576)
        M, N = selected_attention.shape

        # ç‰¹å¾1: æ³¨æ„åŠ›ç†µåˆ†æ
        attention_probs = torch.softmax(selected_attention, dim=-1)
        entropy = -(attention_probs * torch.log(attention_probs + 1e-8)).sum(dim=-1)  # (M,)
        avg_entropy = entropy.mean().item()

        # ç‰¹å¾2: ç¨€ç–æ€§åˆ†æ
        sparsity = (attention_probs ** 2).sum(dim=-1)  # (M,)
        avg_sparsity = sparsity.mean().item()

        # ç‰¹å¾3: å¤´å¤šæ ·æ€§åˆ†æ
        def compute_head_diversity(attention_matrix):
            normalized = F.normalize(attention_matrix, p=2, dim=-1)
            similarity_matrix = torch.mm(normalized, normalized.t())
            # å»æ‰å¯¹è§’çº¿è‡ªç›¸ä¼¼
            if similarity_matrix.size(0) > 1:
                mask = ~torch.eye(similarity_matrix.size(0), dtype=torch.bool, device=similarity_matrix.device)
                avg_similarity = similarity_matrix[mask].mean().item()
                return 1 - avg_similarity  # å¤šæ ·æ€§ = 1 - ç›¸ä¼¼æ€§
            else:
                return 0.5  # å•ä¸ªå¤´æ—¶è¿”å›ä¸­æ€§å€¼

        diversity = compute_head_diversity(selected_attention)

        # ç‰¹å¾4: æ³¨æ„åŠ›é›†ä¸­åº¦
        max_attention = selected_attention.max(dim=-1)[0]  # (M,)
        avg_concentration = max_attention.mean().item()

        # ç‰¹å¾5: æ³¨æ„åŠ›åˆ†å¸ƒç¨³å®šæ€§
        attention_std = selected_attention.std(dim=-1).mean().item()

        if self.debug_mode:
            print(f"   Attention Analysis: entropy={avg_entropy:.3f}, sparsity={avg_sparsity:.3f}, "
                  f"diversity={diversity:.3f}, concentration={avg_concentration:.3f}, stability={attention_std:.3f}")

        # ç­–ç•¥å†³ç­–æ ‘ (åŸºäºå®éªŒæ•°æ®ä¼˜åŒ–)
        if avg_entropy > 2.5 and diversity > 0.7:
            # é«˜ç†µ + é«˜å¤šæ ·æ€§ â†’ å¤æ‚å¤šç›®æ ‡åœºæ™¯
            if avg_concentration > 0.3:
                strategy = 'graph_based'      # å¤æ‚å…³ç³»ï¼Œç”¨å›¾æ–¹æ³•
            else:
                strategy = 'multi_objective'  # å¤šç›®æ ‡ä¼˜åŒ–

        elif avg_sparsity > 0.8 and avg_concentration > 0.25:
            # é«˜ç¨€ç– + é«˜é›†ä¸­ â†’ æ˜ç¡®ç›®æ ‡åœºæ™¯
            strategy = 'sparsity'  # å®éªŒæ˜¾ç¤ºsparsityåœ¨è¿™ç§æƒ…å†µä¸‹æœ€ä¼˜

        elif avg_entropy < 1.8 and attention_std < 0.1:
            # ä½ç†µ + ç¨³å®š â†’ ç®€å•åœºæ™¯
            strategy = 'hierarchical'  # ç®€å•åœºæ™¯ç”¨åˆ†å±‚æ–¹æ³•

        elif diversity < 0.3:
            # ä½å¤šæ ·æ€§ â†’ å¤´ä¹‹é—´ç›¸ä¼¼ï¼Œéœ€è¦åŒºåˆ†
            strategy = 'attention_range'  # ç”¨rangeæ¥åŒºåˆ†

        elif avg_concentration < 0.15:
            # ä½é›†ä¸­åº¦ â†’ æ³¨æ„åŠ›åˆ†æ•£
            strategy = 'top_k_sum'  # èšç„¦top-k

        else:
            # ä¸­ç­‰å¤æ‚åº¦ â†’ å¹³è¡¡ç­–ç•¥
            strategy = 'multi_objective'

        if self.debug_mode:
            print(f"   Selected strategy: {strategy}")
        return strategy

    # ================== Step 3: ç²¾ç­›é€‰ - ç­–ç•¥åº”ç”¨ ==================

    def apply_refined_selection(self, selected_attention, strategy, target_heads):
        """
        ç²¾ç­›é€‰ï¼šåœ¨å€™é€‰å¤´ä¸­åº”ç”¨ç‰¹å®šç­–ç•¥

        è¾“å…¥: selected_attention (M, 576), strategy (str), target_heads (int)
        è¾“å‡º: refined_indices (ç›¸å¯¹äºselected_attentionçš„ç´¢å¼•)
        """
        # ç¡®ä¿target_headsä¸è¶…è¿‡å¯ç”¨å¤´æ•°
        target_heads = min(target_heads, selected_attention.size(0))

        # æ ¹æ®ç­–ç•¥è®¡ç®—ç²¾ç»†åˆ†æ•°
        if strategy == 'sparsity':
            scores = self.compute_sparsity_scores(selected_attention)
        elif strategy == 'graph_based':
            scores = self.compute_graph_scores(selected_attention)
        elif strategy == 'hierarchical':
            scores = self.compute_hierarchical_scores(selected_attention)
        elif strategy == 'multi_objective':
            scores = self.compute_multi_objective_scores(selected_attention)
        elif strategy == 'attention_range':
            scores = self.compute_attention_range_scores(selected_attention)
        elif strategy == 'top_k_sum':
            scores = self.compute_top_k_sum_scores(selected_attention)
        elif strategy == 'max_attention':
            scores = self.compute_max_attention_scores(selected_attention)
        else:
            # fallback
            scores = selected_attention.sum(dim=-1)

        # é€‰æ‹©top-k
        refined_indices = scores.topk(k=target_heads).indices
        return refined_indices

    def compute_sparsity_scores(self, attention):
        """è®¡ç®—ç¨€ç–æ€§åˆ†æ•° - ä¼˜åŒ–ç‰ˆæœ¬"""
        attention_probs = torch.softmax(attention, dim=-1)
        # L2èŒƒæ•° + æœ€å¤§å€¼æƒé‡
        l2_norm = (attention_probs ** 2).sum(dim=-1)
        max_weight = attention_probs.max(dim=-1)[0]
        return 0.7 * l2_norm + 0.3 * max_weight

    def compute_graph_scores(self, attention):
        """è®¡ç®—å›¾åˆ†æ•° - ç®€åŒ–ç‰ˆæœ¬"""
        if attention.size(0) <= 1:
            return attention.sum(dim=-1)

        # è®¡ç®—èŠ‚ç‚¹ä¸­å¿ƒæ€§
        normalized = F.normalize(attention, p=2, dim=-1)
        similarity = torch.mm(normalized, normalized.t())
        centrality = similarity.sum(dim=-1)  # åº¦ä¸­å¿ƒæ€§

        # ç»“åˆæ³¨æ„åŠ›è´¨é‡
        quality = attention.var(dim=-1)  # æ³¨æ„åŠ›æ–¹å·®
        return 0.6 * centrality + 0.4 * quality

    def compute_hierarchical_scores(self, attention):
        """è®¡ç®—åˆ†å±‚åˆ†æ•°"""
        attention_probs = torch.softmax(attention, dim=-1)

        # ç¬¬ä¸€å±‚: åŸºç¡€æŒ‡æ ‡
        sparsity = (attention_probs ** 2).sum(dim=-1)
        entropy = -(attention_probs * torch.log(attention_probs + 1e-8)).sum(dim=-1)
        entropy_norm = (entropy - entropy.min()) / (entropy.max() - entropy.min() + 1e-8)

        # ç¬¬äºŒå±‚: å¤åˆæŒ‡æ ‡
        kl_div = F.kl_div(torch.log(attention_probs + 1e-8),
                          torch.ones_like(attention_probs) / attention_probs.size(-1),
                          reduction='none').sum(dim=-1)

        return 0.4 * sparsity + 0.3 * (1 - entropy_norm) + 0.3 * kl_div

    def compute_multi_objective_scores(self, attention):
        """è®¡ç®—å¤šç›®æ ‡åˆ†æ•°"""
        attention_probs = torch.softmax(attention, dim=-1)

        # ç›®æ ‡1: ä¿¡æ¯ä¸°å¯Œåº¦
        entropy = -(attention_probs * torch.log(attention_probs + 1e-8)).sum(dim=-1)

        # ç›®æ ‡2: ç¨€ç–æ€§
        sparsity = (attention_probs ** 2).sum(dim=-1)

        # ç›®æ ‡3: é›†ä¸­åº¦
        max_attn = attention_probs.max(dim=-1)[0]

        # å½’ä¸€åŒ–å¹¶ç»„åˆ
        entropy_norm = (entropy - entropy.min()) / (entropy.max() - entropy.min() + 1e-8)
        sparsity_norm = (sparsity - sparsity.min()) / (sparsity.max() - sparsity.min() + 1e-8)
        max_norm = (max_attn - max_attn.min()) / (max_attn.max() - max_attn.min() + 1e-8)

        return 0.4 * entropy_norm + 0.3 * sparsity_norm + 0.3 * max_norm

    def compute_attention_range_scores(self, attention):
        """è®¡ç®—æ³¨æ„åŠ›èŒƒå›´åˆ†æ•°"""
        return attention.max(dim=-1)[0] - attention.min(dim=-1)[0]

    def compute_top_k_sum_scores(self, attention):
        """è®¡ç®—top-kå’Œåˆ†æ•°"""
        k = min(10, attention.size(-1))
        topk_attention = attention.topk(k=k, dim=-1)[0]
        return topk_attention.sum(dim=-1)

    def compute_max_attention_scores(self, attention):
        """è®¡ç®—æœ€å¤§æ³¨æ„åŠ›åˆ†æ•°"""
        return attention.max(dim=-1)[0]

    # ================== Step 4: è‡ªé€‚åº”æ•°é‡è°ƒæ•´ ==================

    def adaptive_head_count(self, refined_scores, base_target):
        """
        è‡ªé€‚åº”å¤´æ•°é‡ï¼šæ ¹æ®è´¨é‡åˆ†å¸ƒåŠ¨æ€è°ƒæ•´æ•°é‡
        """
        if len(refined_scores) <= 1:
            return len(refined_scores)

        # åˆ†æåˆ†æ•°åˆ†å¸ƒ
        sorted_scores, _ = torch.sort(refined_scores, descending=True)

        # æ–¹æ³•1: Gap-based stopping
        gaps = sorted_scores[:-1] - sorted_scores[1:]
        mean_gap = gaps.mean()
        std_gap = gaps.std()

        # æ‰¾åˆ°æ˜¾è‘—gap
        significant_gaps = gaps > (mean_gap + std_gap)
        if significant_gaps.any():
            stop_idx = significant_gaps.nonzero()[0].item() + 1
            adaptive_count = min(stop_idx, base_target)
        else:
            adaptive_count = base_target

        # æ–¹æ³•2: Quality threshold
        mean_score = sorted_scores.mean()
        std_score = sorted_scores.std()
        threshold = mean_score - 0.5 * std_score

        quality_count = (sorted_scores > threshold).sum().item()

        # ç»¼åˆå†³ç­–
        final_count = min(
            max(adaptive_count, quality_count // 2),  # è‡³å°‘ä¿è¯è´¨é‡
            base_target,  # ä¸è¶…è¿‡ç›®æ ‡
            len(sorted_scores)  # ä¸è¶…è¿‡å¯ç”¨æ•°é‡
        )

        return max(final_count, 1)  # è‡³å°‘ä¿è¯1ä¸ªå¤´

    # ================== ä¸»è¦çš„åŠ¨æ€é€‰æ‹©æµç¨‹ ==================

    def hierarchical_dynamic_selection(self, image_attention):
        """
        å®Œæ•´çš„å±‚æ¬¡åŒ–åŠ¨æ€é€‰æ‹©æµç¨‹
        """
        if self.debug_mode:
            print(f"ğŸš€ Starting hierarchical dynamic selection...")

        # Step 1: ç²—ç­›é€‰
        visual_head_indices, sensitivity_scores = self.dynamic_visual_head_detection(image_attention)

        # Step 2: ç­–ç•¥åˆ¤æ–­
        strategy = self.adaptive_strategy_selection(image_attention, visual_head_indices)

        # Step 3: ç²¾ç­›é€‰
        selected_attention = image_attention[visual_head_indices]
        refined_indices = self.apply_refined_selection(selected_attention, strategy, self.H)

        # Step 4: è‡ªé€‚åº”æ•°é‡
        refined_scores = sensitivity_scores[refined_indices]  # ä½¿ç”¨ä¹‹å‰çš„sensitivity scores
        final_count = self.adaptive_head_count(refined_scores, self.H)

        # è·å–æœ€ç»ˆå¤´ç´¢å¼•
        final_refined_indices = refined_indices[:final_count]
        final_head_indices = visual_head_indices[final_refined_indices]

        if self.debug_mode:
            print(f"âœ… Final result: {len(final_head_indices)} heads using {strategy} strategy")

        return final_head_indices, strategy

    # ================== Forwardå‡½æ•° - ä¿æŒä¸åŸä»£ç å…¼å®¹ ==================

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutputWithPast]:

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # retrieve input_ids and inputs_embeds
        if input_ids is not None and inputs_embeds is not None:
            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
        elif input_ids is not None:
            batch_size, seq_length = input_ids.shape[:2]
        elif inputs_embeds is not None:
            batch_size, seq_length = inputs_embeds.shape[:2]
        else:
            raise ValueError("You have to specify either input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training:
            if use_cache:
                print("`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...")
                use_cache = False

        past_key_values_length = 0
        if use_cache:
            use_legacy_cache = not isinstance(past_key_values, Cache)
            if use_legacy_cache:
                past_key_values = DynamicCache.from_legacy_cache(past_key_values)
            past_key_values_length = past_key_values.get_usable_length(seq_length)

        if position_ids is None:
            device = input_ids.device if input_ids is not None else inputs_embeds.device
            position_ids = torch.arange(
                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
            )
            position_ids = position_ids.unsqueeze(0)

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if self._use_flash_attention_2:
            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
        elif self._use_sdpa and not output_attentions:
            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
                attention_mask,
                (batch_size, seq_length),
                inputs_embeds,
                past_key_values_length,
            )
        else:
            attention_mask = _prepare_4d_causal_attention_mask(
                attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
            )

        hidden_states = inputs_embeds
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        next_decoder_cache = None

        if seq_length > 1:
            visual_token_length = self.visual_token_length
            visual_token_num = 0

        # Head Selection - è¯»å–é…ç½®
        if self.config.H is not None:
            self.H = self.config.H
            if self.debug_mode:
                print(f"Head_number updated to: {self.H}")

        for decoder_layer in self.layers:
            if output_hidden_states:
                all_hidden_states += (hidden_states,)

            if self.gradient_checkpointing and self.training:
                layer_outputs = self._gradient_checkpointing_func(
                    decoder_layer.__call__,
                    hidden_states,
                    attention_mask,
                    position_ids,
                    past_key_values,
                    output_attentions,
                    use_cache,
                )
            else:
                # åŠ¨æ€Headé€‰æ‹©çš„æ ¸å¿ƒé€»è¾‘
                if seq_length > 1:
                    visual_token_num += visual_token_length

                    if (decoder_layer.self_attn.layer_idx + 1) == self.K:
                        attn_mask = torch.ones((batch_size, seq_length), device=hidden_states.device)
                        attn_mask = _prepare_4d_causal_attention_mask(attn_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length)
                        layer_outputs = decoder_layer(
                            hidden_states,
                            attention_mask=attn_mask,
                            position_ids=position_ids,
                            past_key_value=past_key_values,
                            output_attentions=True,
                            use_cache=use_cache,
                        )
                        hidden_states = layer_outputs[0]
                        last_attention = layer_outputs[1]

                        # è·å–æœ€åä¸€ä¸ªquery tokençš„æ³¨æ„åŠ›
                        image_attention = self.get_last_query_attention(last_attention)  # (H, N)

                        # ä½¿ç”¨åŠ¨æ€é€‰æ‹©ç­–ç•¥
                        if self.enable_dynamic_selection:
                            visual_head_index, selected_strategy = self.hierarchical_dynamic_selection(image_attention)
                            if self.debug_mode:
                                print(f"ğŸ¯ Dynamic selection: {len(visual_head_index)} heads, strategy: {selected_strategy}")
                        else:
                            # Fallback to simple sum selection
                            head_attention = image_attention.sum(dim=-1)
                            visual_head_index = head_attention.topk(k=self.H).indices
                            if self.debug_mode:
                                print(f"ğŸ”§ Fallback: using simple sum selection with {self.H} heads")

                        # ä½¿ç”¨é€‰ä¸­çš„å¤´è®¡ç®—visual tokené‡è¦æ€§
                        image_attention = image_attention[visual_head_index].mean(dim=0)  # (N)

                        visual_token_length = self.R
                        visual_token_index = torch.topk(image_attention, k=self.R).indices # (R)
                        visual_token_index = torch.sort(visual_token_index).values # (R)

                        device = hidden_states.device
                        full_token_index = torch.cat((
                            torch.arange(self.system_prompt_length, device=device),
                            visual_token_index + self.system_prompt_length,
                            torch.arange(self.system_prompt_length+self.visual_token_length, seq_length, device=device),
                        )) # (T)

                        # get tokens by index
                        hidden_states = hidden_states[:,full_token_index]
                        position_ids = full_token_index.unsqueeze(0)

                        layer_outputs = (hidden_states, layer_outputs[2])

                    else:
                        layer_outputs = decoder_layer(
                            hidden_states,
                            attention_mask=attention_mask,
                            position_ids=position_ids,
                            past_key_value=past_key_values,
                            output_attentions=output_attentions,
                            use_cache=use_cache,
                        )

                else:
                    layer_outputs = decoder_layer(
                        hidden_states,
                        attention_mask=attention_mask,
                        position_ids=position_ids,
                        past_key_value=past_key_values,
                        output_attentions=output_attentions,
                        use_cache=use_cache,
                    )

            hidden_states = layer_outputs[0]

            if use_cache:
                next_decoder_cache = layer_outputs[2 if output_attentions else 1]

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        if seq_length > 1:
            self.visual_token_num = visual_token_num / len(self.layers)

        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        next_cache = None
        if use_cache:
            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache
        if not return_dict:
            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=next_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )